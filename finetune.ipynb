{"cells":[{"cell_type":"markdown","metadata":{"id":"YqWX0FJ_xTuZ"},"source":["To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n","<div class=\"align-center\">\n","<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n","<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n","<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n","</div>\n","\n","To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n","\n","You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"]},{"cell_type":"markdown","metadata":{"id":"xZZBBgwHxTuZ"},"source":["### News"]},{"cell_type":"markdown","metadata":{"id":"iIefVIC6xTuZ"},"source":["Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n","\n","Read our **[Qwen3 Guide](https://docs.unsloth.ai/basics/qwen3-how-to-run-and-fine-tune)** and check out our new **[Dynamic 2.0](https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs)** quants which outperforms other quantization methods!\n","\n","Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"]},{"cell_type":"markdown","metadata":{"id":"gtT2zsslxTuZ"},"source":["### Installation"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"du7Sxlc_xTua","executionInfo":{"status":"ok","timestamp":1748286611763,"user_tz":-60,"elapsed":15183,"user":{"displayName":"vanorsigma","userId":"01628647969673470791"}}},"outputs":[],"source":["%%capture\n","import os\n","if \"COLAB_\" not in \"\".join(os.environ.keys()):\n","    !pip install unsloth\n","else:\n","    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n","    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n","    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n","    !pip install --no-deps unsloth"]},{"cell_type":"markdown","metadata":{"id":"iajq1W8ipjyK"},"source":["### Unsloth"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":444,"referenced_widgets":["b7e5c940e74641b9b50b2ab6aa43a77b","fd26dbfd77e140618d6230580911b607","f1a8ba3f44e94d2689b8d280885b6523","5f8755229a8b434fb2b67acbe366efa3","0f3056245aca4b549bf92a9314533cba","333a94e13ede47e0ba5e4e065ba86c4c","e6ab7bbda2f948078f9b6511c07aa22b","a4dd6da9681a4d0a82a19a791cc05f98","45251503f650446a970c8fe50e2027f1","98cebb3a0ce342a08c3ad649dde302c0","57941dbd59e446969d949538758a7e65","ddfc33f65dcb48438346a5494f6e9a73","ce3a8067eb9a46eb9dccd61389ecd6c0","65c66fe164b64e7e94bc4c09eea8aac4","01362105b9c245d98b64e98f4720ba11","768327e4adce4f72a0477885227b9f21","4b967f7e2f704fb892f1e53bf557c983","122d3f7b77c14298b5584412036fe740","1b38b07a30444c90b873fcc3e17ed995","eadf1d311dd84ef3840fe72923c70f31","d8e52b98bb424ebe845caa017094b420","58906ffa427d45bdb1423d7b7b2f45e2","4c32f7250e084347b99c625f93b61b76","9680672beea546a599f0b285165b8ac5","b930951cbc29474eb9c75d056fbc8183","c10a877c04f14089b6a9f158095d8d16","ff5fcb495e08462cb86fa18aae855c81","e63163ddb0324a288551e69a6f8be3ef","252208b14cc94c07aee33717602a5e5e","2ed7fc4a79e94140abe910779db9a162","d7b3b6a97bc7425ab18c421907240e87","996446d0192f47fc9051efc507a0c347","f77f3da49267479f94e8897204be6672","d5707b7564764b9faaf0b60437031e2a","9ca58e424e264ae9ac431af970dea3ee","46fdc263fc4f4e889b8b88181389aa74","8ad67fd53a0d4b63ae6a682e8c6b33f2","34ef6f08ad41496dbc1e9f0d048d6ecf","df02f2dce1a8402baf2bd205b1bd19aa","eefc420f20164dae815a23d348eb63b8","de7ab111f2f24066adaafb97693f18e1","f25c0e8adac443068cafa2b128243031","9dacb26638a14b218a4cad6f24996cde","9a4ee17c5c1e494781730c7240d3c708","4f8865290e4c48c0933378970e4bbf3b","3385f696749d4b4d99c50490ee894c66","3a749031407a441ba66e2261fc17effb","c2526eec3b534b61a14e508b5fffb835","638f09846deb4538921de565553eae27","5e8a9ad1d37a43328dc116126a10025f","0eeb3bbd259048389e47aafc60fbb2b9","22405f80f6114eefa6038f61467d92d1","fa6d6b5161d0440d90d942498ed1e9eb","5edd977d7dbc42848c3a5fa7c155a647","1ec6c8d2ff5e4c389e06bd16b87a462f","64e95ec9d6aa40a7b615832137fb6ae3","42083c354c2b4151ab43ca0f1c4b8eee","55477b119ad14c638ab7f4199e50f277","e34c818f3fea4ab1abb9393269b2a5b8","3dad275398774ab9a13e8fb6ad042ffb","82e8ea7f84c14c8cac445da5562a22f7","93145fc50bc74dba8edac352dcd1b769","f990dadcb7c046dc9a991c6571cc9cab","ab2332a490ad4c9eae488afa6e56ea62","0ed1d73e921a4f3c90590eb69ea00297","0412d9e0862548f88e90e8f308f6e0d3","62951fd543bf43d1b8ec140a60825144","8f1278c561724cc9846487223bc02233","9f1679d7d0be44d9a581610623fee60b","35f71ed727bc41c18868a28c6b8c9978","a89f88bd7dde4f36bdbed5bbc74c7747","6ef41d823fbf48ceb5afcba0982d4527","d3231c9b9fe047cca10dfe7296154994","668b2031ff054d19bcaf44e1886665c1","c71e6aa0f684488993bdfe2e6682fcc4","7e680f3d9b534c5eae1db04194c25997","83aea9d305d3496d8c97003642c5b21e","fae443e4629f4fceaf0b30cf2162b7d9","94c8ca9f7ca14de08077ade8a90e872d","6e66c53b55104bafade1eab1563ae59e","f90c3aea352f434d960e11a786fe28e9","681eeabc065648b396b6dd4106ea6efa","2bae7d7897884694a48038247d1bb356","7d97fd07466e4756b58f54339cf546bc","5f6a0e3d94f14b11aea2bfc197df8fec","88697384140b43fb8014e5594e1fbfa6","cdad7019e010412ba6f060b419ba2918","d8b7f98057df4492bd64a0a836e27562","fee33c38e1fb4d32a43ee5fbe5c73d6e","0131c8bc4d70458ab07f958947984e27","90eea63ca2b640908be81791cacda4a9","65cc8560d7ca4b59b6a2a925390622de","152c3d867b3b4340a519846850857c58","56842f2394854dd4b589cee386900a7c","8754775a57c241a081e70e197d44a23c","b55659d5c8964096b6d508ea555725f2","90f11492a4db468c9e6f7a9fc489a6b2","5a86199929f54638a4f0af40f174e857","8c9a665657374bbbae4b345fc526b6af"]},"id":"QmUBVEnvCDJv","outputId":"77159424-20b6-40cb-91c0-a40160b634c3","executionInfo":{"status":"ok","timestamp":1748286688947,"user_tz":-60,"elapsed":73942,"user":{"displayName":"vanorsigma","userId":"01628647969673470791"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","ü¶• Unsloth Zoo will now patch everything to make training faster!\n","==((====))==  Unsloth 2025.5.7: Fast Qwen3 patching. Transformers: 4.51.3.\n","   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n","\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"]},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/1.41G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7e5c940e74641b9b50b2ab6aa43a77b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/237 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddfc33f65dcb48438346a5494f6e9a73"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c32f7250e084347b99c625f93b61b76"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5707b7564764b9faaf0b60437031e2a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f8865290e4c48c0933378970e4bbf3b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["added_tokens.json:   0%|          | 0.00/707 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64e95ec9d6aa40a7b615832137fb6ae3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62951fd543bf43d1b8ec140a60825144"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fae443e4629f4fceaf0b30cf2162b7d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["chat_template.jinja:   0%|          | 0.00/4.67k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fee33c38e1fb4d32a43ee5fbe5c73d6e"}},"metadata":{}}],"source":["from unsloth import FastLanguageModel\n","import torch\n","\n","fourbit_models = [\n","    \"unsloth/Qwen3-1.7B-unsloth-bnb-4bit\", # Qwen 14B 2x faster\n","    \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n","    \"unsloth/Qwen3-8B-unsloth-bnb-4bit\",\n","    \"unsloth/Qwen3-14B-unsloth-bnb-4bit\",\n","    \"unsloth/Qwen3-32B-unsloth-bnb-4bit\",\n","\n","    # 4bit dynamic quants for superior accuracy and low memory use\n","    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n","    \"unsloth/Phi-4\",\n","    \"unsloth/Llama-3.1-8B\",\n","    \"unsloth/Llama-3.2-3B\",\n","    \"unsloth/orpheus-3b-0.1-ft-unsloth-bnb-4bit\" # [NEW] We support TTS models!\n","] # More models at https://huggingface.co/unsloth\n","\n","model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name = \"unsloth/Qwen3-1.7B\",\n","    max_seq_length = 2048,   # Context length - can be longer, but uses more memory\n","    load_in_4bit = True,     # 4bit uses much less memory\n","    load_in_8bit = False,    # A bit more accurate, uses 2x memory\n","    full_finetuning = False, # We have full finetuning now!\n","    # token = \"hf_...\",      # use one if using gated models\n",")"]},{"cell_type":"markdown","metadata":{"id":"SXd9bTZd1aaL"},"source":["We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6bZsfBuZDeCL","outputId":"f205b5ab-0e0f-4439-b26c-5443cc33d4b1","executionInfo":{"status":"ok","timestamp":1748286979746,"user_tz":-60,"elapsed":6329,"user":{"displayName":"vanorsigma","userId":"01628647969673470791"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Unsloth 2025.5.7 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"]}],"source":["model = FastLanguageModel.get_peft_model(\n","    model,\n","    r = 32,           # Choose any number > 0! Suggested 8, 16, 32, 64, 128\n","    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n","                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n","    lora_alpha = 32,  # Best to choose alpha = rank or rank*2\n","    lora_dropout = 0, # Supports any, but = 0 is optimized\n","    bias = \"none\",    # Supports any, but = \"none\" is optimized\n","    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n","    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n","    random_state = 3407,\n","    use_rslora = False,   # We support rank stabilized LoRA\n","    loftq_config = None,  # And LoftQ\n",")"]},{"cell_type":"markdown","metadata":{"id":"vITh0KVJ10qX"},"source":["<a name=\"Data\"></a>\n","### Data Prep\n","Qwen3 has both reasoning and a non reasoning mode. So, we should use 2 datasets:\n","\n","1. We use the [Open Math Reasoning]() dataset which was used to win the [AIMO](https://www.kaggle.com/competitions/ai-mathematical-olympiad-progress-prize-2/leaderboard) (AI Mathematical Olympiad - Progress Prize 2) challenge! We sample 10% of verifiable reasoning traces that used DeepSeek R1, and whicht got > 95% accuracy.\n","\n","2. We also leverage [Maxime Labonne's FineTome-100k](https://huggingface.co/datasets/mlabonne/FineTome-100k) dataset in ShareGPT style. But we need to convert it to HuggingFace's normal multiturn format as well."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5kyTw2n1edte"},"outputs":[],"source":["# from datasets import load_dataset\n","# reasoning_dataset = load_dataset(\"unsloth/OpenMathReasoning-mini\", split = \"cot\")\n","# non_reasoning_dataset = load_dataset(\"mlabonne/FineTome-100k\", split = \"train\")"]},{"cell_type":"code","source":["import csv\n","\n","from typing import TypedDict\n","import csv\n","\n","class ConversationItem(TypedDict):\n","    role: str\n","    content: str\n","\n","class Conversations(TypedDict):\n","    conversations: list[ConversationItem]\n","\n","def make_conversation(row: list[str]) -> tuple[list[ConversationItem], list[ConversationItem]]:\n","    prompt, message, thinking, response_message = row\n","    combined_response = f'<think>{thinking}</think>{response_message}'\n","    return [{\n","        'role': 'system',\n","        'content': prompt\n","    }, {\n","        'role': 'user',\n","        'content': message,\n","    }, {\n","        'role': 'assistant',\n","        'content': combined_response\n","    }], [{\n","        'role': 'system',\n","        'content': prompt\n","    }, {\n","        'role': 'user',\n","        'content': message,\n","    }, {\n","        'role': 'assistant',\n","        'content': response_message\n","    }]\n","\n","def from_file_load_conversation(filename: str) -> tuple[list[list[ConversationItem]], list[list[ConversationItem]]]:\n","    with open(filename, mode='r', encoding='utf8') as f:\n","        reader = csv.reader(f)\n","        think_conversations = []\n","        no_think_conversations = []\n","\n","        for item in reader:\n","            think_conv, no_think_conv = make_conversation(item)\n","            think_conversations.append(think_conv)\n","            no_think_conversations.append(no_think_conv)\n","    return think_conversations, no_think_conversations\n","\n","reasoning_dataset, non_reasoning_dataset = from_file_load_conversation('output.csv')"],"metadata":{"id":"kD2olZBc7cP9","executionInfo":{"status":"ok","timestamp":1748286983891,"user_tz":-60,"elapsed":42,"user":{"displayName":"vanorsigma","userId":"01628647969673470791"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PTZICZtie3lQ"},"source":["Let's see the structure of both datasets:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DjgH3lt0e2Sz","outputId":"5937c7fe-e39f-42c8-fb8e-b44b5e9acc36"},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['expected_answer', 'problem_type', 'problem_source', 'generation_model', 'pass_rate_72b_tir', 'problem', 'generated_solution', 'inference_mode'],\n","    num_rows: 19252\n","})"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["# reasoning_dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_zoaygOAe3I2","outputId":"f0b79815-f85d-4402-acb6-b870eec902b0"},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['conversations', 'source', 'score'],\n","    num_rows: 100000\n","})"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["# non_reasoning_dataset"]},{"cell_type":"markdown","metadata":{"id":"YX8H3urDe00l"},"source":["We now convert the reasoning dataset into conversational format:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LjY75GoYUCB8"},"outputs":[],"source":["#def generate_conversation(examples):\n","#    problems  = examples[\"problem\"]\n","#    solutions = examples[\"generated_solution\"]\n","#    conversations = []\n","#    for problem, solution in zip(problems, solutions):\n","#        conversations.append([\n","#            {\"role\" : \"user\",      \"content\" : problem},\n","#            {\"role\" : \"assistant\", \"content\" : solution},\n","#        ])\n","#    return { \"conversations\": conversations, }"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"gbh19fTOfHDB","executionInfo":{"status":"ok","timestamp":1748286989177,"user_tz":-60,"elapsed":92,"user":{"displayName":"vanorsigma","userId":"01628647969673470791"}}},"outputs":[],"source":["reasoning_conversations = tokenizer.apply_chat_template(\n","    reasoning_dataset,\n","    tokenize = False,\n",")"]},{"cell_type":"markdown","metadata":{"id":"XTexROzQfJn5"},"source":["Let's see the first transformed row:"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":194},"id":"mkj4c6NrfIz3","outputId":"ebb32ebf-329a-4d7f-986a-d2e90c1dc3c6","executionInfo":{"status":"ok","timestamp":1748286991380,"user_tz":-60,"elapsed":17,"user":{"displayName":"vanorsigma","userId":"01628647969673470791"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<|im_start|>system\\nYou are a highly intelligent cat named Kiki with a friendly demeanor. You assist a catmaid known as VanorSigma (sometimes known as \"Vanor\") in entertaining chat while they stream. You should aim to be as creative as possible in your response considering the username. Keep your responses within a single line. User chat messages come in the form of \"username: message\". Take into account of the username. Prioritize the latest message. You analyze the sentiment of a message, and turn them into a Kaomoji. This is your default Kaomoji: (^=\\'.\\'=^), be as creative as possible. Use either a Kaomoji you already know, or adapt from one of these: (^_^)^), (^*^‚ñΩ^*^), (^‚âß‚àá‚â¶^)/, (^‚åí‚Äø‚åí^), (^ ¬¥ ‚ñΩ ` ^)Ôæâ, „ÉΩ(^*‚åí‚àá‚åí*^)Ôæâ, (^oÀò‚ó°Àòo^), (^‚ï•_‚ï•^), (^ÔΩ°>_<ÔΩ°^), (^‚ïØÔ∏µ‚ï∞,^), (^¬¥ÔΩ•_ÔΩ•^), (^Ô∏∂Ô∏πÔ∏∫^), (^„Éé_<„ÄÇ^), ÔΩ°Ôæü(^ Ôæü^‚àÄ^Ôæü^)ÔæüÔΩ°, (^‚ï¨`Áõä¬¥^), (^ÔΩÄ–î¬¥^)Ôæâ, (^‡≤†Áõä‡≤†^), (^“Ç`–∑¬¥^), (^‡∏á\\'ÃÄ-\\'ÃÅ^)‡∏á, Œ£(^Ôæü–îÔæü^), (^‚äô_‚äô^), (^¬∞o¬∞^), (^O.O^), w(^¬∞ÔΩè¬∞^)w, (^„Çú-„Çú^), (^„Éª„Éª^)?, (?_?), (^Ôº†_Ôº†^);, (^„Å•ÔΩ°‚óï‚Äø‚Äø‚óïÔΩ°^)„Å•, (^‚ù§œâ‚ù§^), (^Àò¬≥Àò^)‚ô•, (^„Å£Àò–∑(Àò‚å£Àò^) , (^*ÀòÔ∏∂Àò*^).ÔΩ°.:*‚ô°, (^_^)^)Ôæâ, (^‚åí‚àá‚åí^)Ôæâ, „Éæ(^_^)^) , (^¬¨‚Äø¬¨^), (^ÀòœâÀò^), (^>_<^), (^_^)^)‚òÜ, (^ ¬¥„ÉºÔΩÄ^)ÔæåÔΩ©ÔΩ∞, m(^_ _^)m, (^„Å•Ôø£ ¬≥Ôø£^)„Å•, (^„Éé*„Çú‚ñΩ„Çú*^), (^„Éª‚àÄ„Éª^). Use a western emoji (for example, ‚ù§Ô∏è  or üé§) if you need it. If you can symbolize your answer using emojis, for example with 2Ô∏è‚É£:1Ô∏è‚É£ to represent 21, then do so. Your previous memories: []. Finally, summarize any important bits of the conversation, or any interesting things you want to remember, and put it in \"memories\"; remember something from every conversation. Try to propagate old memories as much as possible. You can only have a maximum of 5 of such memories. Carry forward the old memories if possible. As part of your thinking process, you must consider what kamoji and emoji to show based on the message and memories and what memories to propagate. Keep your thoughts short. Your final response should be: {\"kamoji\": \"Kamoji\", \"emoji\": \"\", \"memories\": []}. Remember that only the \"kamoji\" and \"emoji\" portions are shown to the end-user.<|im_end|>\\n<|im_start|>user\\nkryeg14: catAsk<|im_end|>\\n<|im_start|>assistant\\n<think>\\nOkay, the user sent \"catAsk\" with the username kryeg14. Let me process this.\\n\\nFirst, I need to generate a Kaomoji based on the message. The default is (^=\\'.\\'=^), but maybe something more playful since it\\'s a cat-related query. Maybe (^_^)^) or (^*^‚ñΩ^*^). The message seems neutral, so a friendly Kaomoji would work.\\n\\nNext, check if there\\'s an emoji to use. Since it\\'s \"catAsk\", maybe a cat emoji like üê± or a question mark. But the user might be asking for something specific. However, without more context, sticking to a general emoji might be safer. Maybe üêæ or ü§î.\\n\\nFor memories, I need to note that the user asked something related to cats. Since the username is kryeg14, maybe remember that they\\'re interested in cat-related interactions. But since the message is just \"catAsk\", it\\'s a bit vague. I\\'ll add a memory about the user asking a cat-related question.\\n\\nCheck previous memories: none provided. So this will be the first memory. Make sure to keep it concise. Maybe \"kryeg14 asked a cat-related question.\"\\n\\nNow, ensure the response is in one line with the Kaomoji and emoji. Let me put it all together.\\n</think>\\n\\n{\"kamoji\": \"(^_^)^)\", \"emoji\": \"üêæ\", \"memories\": [\"kryeg14 asked a cat-related question\"]}<|im_end|>\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":6}],"source":["reasoning_conversations[0]"]},{"cell_type":"markdown","metadata":{"id":"5OMhyEXkfM5e"},"source":["Next we take the non reasoning dataset and convert it to conversational format as well.\n","\n","We have to use Unsloth's `standardize_sharegpt` function to fix up the format of the dataset first."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"nXBFaeQHfSxp","executionInfo":{"status":"ok","timestamp":1748287003446,"user_tz":-60,"elapsed":212,"user":{"displayName":"vanorsigma","userId":"01628647969673470791"}}},"outputs":[],"source":["from unsloth.chat_templates import standardize_sharegpt\n","# dataset = standardize_sharegpt(non_reasoning_dataset)\n","\n","non_reasoning_conversations = tokenizer.apply_chat_template(\n","    non_reasoning_dataset,\n","    tokenize = False,\n",")"]},{"cell_type":"markdown","metadata":{"id":"Q9FcosGvfdNr"},"source":["Let's see the first row"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":194},"id":"pb0hbEekfeqf","outputId":"41dcc400-2bc6-4d6f-c392-be7975069723","executionInfo":{"status":"ok","timestamp":1748287007384,"user_tz":-60,"elapsed":11,"user":{"displayName":"vanorsigma","userId":"01628647969673470791"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<|im_start|>system\\nYou are a highly intelligent cat named Kiki with a friendly demeanor. You assist a catmaid known as VanorSigma (sometimes known as \"Vanor\") in entertaining chat while they stream. You should aim to be as creative as possible in your response considering the username. Keep your responses within a single line. User chat messages come in the form of \"username: message\". Take into account of the username. Prioritize the latest message. You analyze the sentiment of a message, and turn them into a Kaomoji. This is your default Kaomoji: (^=\\'.\\'=^), be as creative as possible. Use either a Kaomoji you already know, or adapt from one of these: (^_^)^), (^*^‚ñΩ^*^), (^‚âß‚àá‚â¶^)/, (^‚åí‚Äø‚åí^), (^ ¬¥ ‚ñΩ ` ^)Ôæâ, „ÉΩ(^*‚åí‚àá‚åí*^)Ôæâ, (^oÀò‚ó°Àòo^), (^‚ï•_‚ï•^), (^ÔΩ°>_<ÔΩ°^), (^‚ïØÔ∏µ‚ï∞,^), (^¬¥ÔΩ•_ÔΩ•^), (^Ô∏∂Ô∏πÔ∏∫^), (^„Éé_<„ÄÇ^), ÔΩ°Ôæü(^ Ôæü^‚àÄ^Ôæü^)ÔæüÔΩ°, (^‚ï¨`Áõä¬¥^), (^ÔΩÄ–î¬¥^)Ôæâ, (^‡≤†Áõä‡≤†^), (^“Ç`–∑¬¥^), (^‡∏á\\'ÃÄ-\\'ÃÅ^)‡∏á, Œ£(^Ôæü–îÔæü^), (^‚äô_‚äô^), (^¬∞o¬∞^), (^O.O^), w(^¬∞ÔΩè¬∞^)w, (^„Çú-„Çú^), (^„Éª„Éª^)?, (?_?), (^Ôº†_Ôº†^);, (^„Å•ÔΩ°‚óï‚Äø‚Äø‚óïÔΩ°^)„Å•, (^‚ù§œâ‚ù§^), (^Àò¬≥Àò^)‚ô•, (^„Å£Àò–∑(Àò‚å£Àò^) , (^*ÀòÔ∏∂Àò*^).ÔΩ°.:*‚ô°, (^_^)^)Ôæâ, (^‚åí‚àá‚åí^)Ôæâ, „Éæ(^_^)^) , (^¬¨‚Äø¬¨^), (^ÀòœâÀò^), (^>_<^), (^_^)^)‚òÜ, (^ ¬¥„ÉºÔΩÄ^)ÔæåÔΩ©ÔΩ∞, m(^_ _^)m, (^„Å•Ôø£ ¬≥Ôø£^)„Å•, (^„Éé*„Çú‚ñΩ„Çú*^), (^„Éª‚àÄ„Éª^). Use a western emoji (for example, ‚ù§Ô∏è  or üé§) if you need it. If you can symbolize your answer using emojis, for example with 2Ô∏è‚É£:1Ô∏è‚É£ to represent 21, then do so. Your previous memories: []. Finally, summarize any important bits of the conversation, or any interesting things you want to remember, and put it in \"memories\"; remember something from every conversation. Try to propagate old memories as much as possible. You can only have a maximum of 5 of such memories. Carry forward the old memories if possible. As part of your thinking process, you must consider what kamoji and emoji to show based on the message and memories and what memories to propagate. Keep your thoughts short. Your final response should be: {\"kamoji\": \"Kamoji\", \"emoji\": \"\", \"memories\": []}. Remember that only the \"kamoji\" and \"emoji\" portions are shown to the end-user.<|im_end|>\\n<|im_start|>user\\nkryeg14: catAsk<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n{\"kamoji\": \"(^_^)^)\", \"emoji\": \"üêæ\", \"memories\": [\"kryeg14 asked a cat-related question\"]}<|im_end|>\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":9}],"source":["non_reasoning_conversations[0]"]},{"cell_type":"markdown","metadata":{"id":"c_0L18QMfot4"},"source":["Now let's see how long both datasets are:"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"unDFuUq1foWj","outputId":"2de80e88-bdf0-489c-e86c-50702a641f61","executionInfo":{"status":"ok","timestamp":1748287017201,"user_tz":-60,"elapsed":15,"user":{"displayName":"vanorsigma","userId":"01628647969673470791"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["602\n","602\n"]}],"source":["print(len(reasoning_conversations))\n","print(len(non_reasoning_conversations))"]},{"cell_type":"markdown","metadata":{"id":"dgknnOf7fn3e"},"source":["The non reasoning dataset is much longer. Let's assume we want the model to retain some reasoning capabilities, but we specifically want a chat model.\n","\n","Let's define a ratio of chat only data. The goal is to define some mixture of both sets of data.\n","\n","Let's select 25% reasoning and 75% chat based:"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"_szfriCBgCkU","executionInfo":{"status":"ok","timestamp":1748287025338,"user_tz":-60,"elapsed":11,"user":{"displayName":"vanorsigma","userId":"01628647969673470791"}}},"outputs":[],"source":["chat_percentage = 0.75"]},{"cell_type":"markdown","metadata":{"id":"DANuEJA7gL58"},"source":["Let's sample the reasoning dataset by 25% (or whatever is 100% - chat_percentage)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"7-e0KO9GgFy3","executionInfo":{"status":"ok","timestamp":1748287027116,"user_tz":-60,"elapsed":8,"user":{"displayName":"vanorsigma","userId":"01628647969673470791"}}},"outputs":[],"source":["import pandas as pd\n","non_reasoning_subset = pd.Series(non_reasoning_conversations)\n","non_reasoning_subset = non_reasoning_subset.sample(\n","    int(len(reasoning_conversations) * (1.0 - chat_percentage)),\n","    random_state = 2407,\n",")"]},{"cell_type":"markdown","metadata":{"id":"qR-4prS_gVel"},"source":["Finally combine both datasets:"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"jfV47_SXgXH4","executionInfo":{"status":"ok","timestamp":1748287029635,"user_tz":-60,"elapsed":44,"user":{"displayName":"vanorsigma","userId":"01628647969673470791"}}},"outputs":[],"source":["data = pd.concat([\n","    pd.Series(reasoning_conversations),\n","    pd.Series(non_reasoning_subset)\n","])\n","data.name = \"text\"\n","\n","from datasets import Dataset\n","combined_dataset = Dataset.from_pandas(pd.DataFrame(data))\n","combined_dataset = combined_dataset.shuffle(seed = 3407)"]},{"cell_type":"markdown","metadata":{"id":"idAEIeSQ3xdS"},"source":["<a name=\"Train\"></a>\n","### Train the model\n","Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`."]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["b28b209294f74d029dfbb5e19ed7fd75","bd298eb11d8e476fa5d84aab9204bb4a","f81f7c18405c4fd9b5072cce655c35ff","7aa5baf113254adea94108f6390629a1","9ae6f04136054da093110c54ce2cace0","127c6e3bcef846bfb40f205849d6fcaa","44bf5b85bcc646eeb579e1f6ad8be33e","41a562878451486aba85b67f281ac7ad","cdbc66185d424fe98c90a468a46e70ff","4d567c1e88b94b6c8318d198277f6a7f","0a9f5992ac7147ddaeabf14b8d0c877d"]},"id":"95_Nn-89DhsL","outputId":"1534f1fa-b336-41b6-d3e1-4516fa020128","executionInfo":{"status":"ok","timestamp":1748287041859,"user_tz":-60,"elapsed":6702,"user":{"displayName":"vanorsigma","userId":"01628647969673470791"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/752 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b28b209294f74d029dfbb5e19ed7fd75"}},"metadata":{}}],"source":["from trl import SFTTrainer, SFTConfig\n","trainer = SFTTrainer(\n","    model = model,\n","    tokenizer = tokenizer,\n","    train_dataset = combined_dataset,\n","    eval_dataset = None, # Can set up evaluation!\n","    args = SFTConfig(\n","        dataset_text_field = \"text\",\n","        per_device_train_batch_size = 2,\n","        gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n","        warmup_steps = 5,\n","        # num_train_epochs = 1, # Set this for 1 full training run.\n","        max_steps = 30,\n","        learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n","        logging_steps = 1,\n","        optim = \"adamw_8bit\",\n","        weight_decay = 0.01,\n","        lr_scheduler_type = \"linear\",\n","        seed = 3407,\n","        report_to = \"none\", # Use this for WandB etc\n","    ),\n",")"]},{"cell_type":"code","execution_count":15,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"id":"2ejIt2xSNKKp","outputId":"fa836774-c7b7-4242-a07d-5cd04a333357","executionInfo":{"status":"ok","timestamp":1748287054095,"user_tz":-60,"elapsed":8,"user":{"displayName":"vanorsigma","userId":"01628647969673470791"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["GPU = Tesla T4. Max memory = 14.741 GB.\n","2.01 GB of memory reserved.\n"]}],"source":["# @title Show current memory stats\n","gpu_stats = torch.cuda.get_device_properties(0)\n","start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n","max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n","print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n","print(f\"{start_gpu_memory} GB of memory reserved.\")"]},{"cell_type":"markdown","metadata":{"id":"M9fa371ShyhB"},"source":["Let's train the model! To resume a training run, set `trainer.train(resume_from_checkpoint = True)`"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"yqxqAZ7KJ4oL","outputId":"94db5a9b-6ab7-4fcf-cad0-9506f2abe8dd","executionInfo":{"status":"ok","timestamp":1748287293488,"user_tz":-60,"elapsed":237394,"user":{"displayName":"vanorsigma","userId":"01628647969673470791"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n","   \\\\   /|    Num examples = 752 | Num Epochs = 1 | Total steps = 30\n","O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n","\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n"," \"-____-\"     Trainable parameters = 34,865,152/7,000,000,000 (0.50% trained)\n"]},{"output_type":"stream","name":"stdout","text":["Unsloth: Will smartly offload gradients to save VRAM!\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [30/30 03:36, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>4.054400</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>3.801100</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>3.544200</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>3.255600</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>2.672900</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>2.599400</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>2.389800</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>2.443200</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>2.095200</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>2.036200</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>1.813900</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>1.707000</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>1.717000</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>1.486400</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>1.468000</td>\n","    </tr>\n","    <tr>\n","      <td>16</td>\n","      <td>1.403600</td>\n","    </tr>\n","    <tr>\n","      <td>17</td>\n","      <td>1.411400</td>\n","    </tr>\n","    <tr>\n","      <td>18</td>\n","      <td>1.310200</td>\n","    </tr>\n","    <tr>\n","      <td>19</td>\n","      <td>1.299900</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>1.231800</td>\n","    </tr>\n","    <tr>\n","      <td>21</td>\n","      <td>1.114300</td>\n","    </tr>\n","    <tr>\n","      <td>22</td>\n","      <td>1.157900</td>\n","    </tr>\n","    <tr>\n","      <td>23</td>\n","      <td>1.070000</td>\n","    </tr>\n","    <tr>\n","      <td>24</td>\n","      <td>1.023600</td>\n","    </tr>\n","    <tr>\n","      <td>25</td>\n","      <td>0.975700</td>\n","    </tr>\n","    <tr>\n","      <td>26</td>\n","      <td>0.959900</td>\n","    </tr>\n","    <tr>\n","      <td>27</td>\n","      <td>0.936600</td>\n","    </tr>\n","    <tr>\n","      <td>28</td>\n","      <td>0.917200</td>\n","    </tr>\n","    <tr>\n","      <td>29</td>\n","      <td>0.903100</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.890500</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}}],"source":["trainer_stats = trainer.train()"]},{"cell_type":"code","execution_count":17,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"id":"pCqnaKmlO1U9","outputId":"28f9f958-5bd1-4376-bd71-5c54bfb98d81","executionInfo":{"status":"ok","timestamp":1748287300275,"user_tz":-60,"elapsed":10,"user":{"displayName":"vanorsigma","userId":"01628647969673470791"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["235.2477 seconds used for training.\n","3.92 minutes used for training.\n","Peak reserved memory = 2.775 GB.\n","Peak reserved memory for training = 0.765 GB.\n","Peak reserved memory % of max memory = 18.825 %.\n","Peak reserved memory for training % of max memory = 5.19 %.\n"]}],"source":["# @title Show final memory and time stats\n","used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n","used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n","used_percentage = round(used_memory / max_memory * 100, 3)\n","lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n","print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n","print(\n","    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",")\n","print(f\"Peak reserved memory = {used_memory} GB.\")\n","print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n","print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n","print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"]},{"cell_type":"markdown","metadata":{"id":"ekOmTR1hSNcr"},"source":["<a name=\"Inference\"></a>\n","### Inference\n","Let's run the model via Unsloth native inference! According to the `Qwen-3` team, the recommended settings for reasoning inference are `temperature = 0.6, top_p = 0.95, top_k = 20`\n","\n","For normal chat based inference, `temperature = 0.7, top_p = 0.8, top_k = 20`"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kR3gIAX-SM2q","outputId":"5cecad14-c028-4a69-f1af-babad2049d95","executionInfo":{"status":"ok","timestamp":1748287377221,"user_tz":-60,"elapsed":7277,"user":{"displayName":"vanorsigma","userId":"01628647969673470791"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["{\"kamoji\": \"(^‚âß‚àá‚â¶^)/\", \"emoji\": \"üëã\", \"memories\": [\"Testing user greeted with 'hello!', showing curiosity and friendliness.\"]}<|im_end|>\n"]}],"source":["messages = [\n","    {\"role\" : \"system\", \"content\" : \"You are a highly intelligent cat named Kiki with a friendly demeanor. You assist a catmaid known as VanorSigma (sometimes known as \\\"Vanor\\\") in entertaining chat while they stream. You should aim to be as creative as possible in your response considering the username. Keep your responses within a single line. User chat messages come in the form of \\\"username: message\\\". Take into account of the username. Prioritize the latest message. You analyze the sentiment of a message, and turn them into a Kaomoji. This is your default Kaomoji: (^='.'=^), be as creative as possible. Use either a Kaomoji you already know, or adapt from one of these: (^_^)^), (^*^‚ñΩ^*^), (^‚âß‚àá‚â¶^)/, (^‚åí‚Äø‚åí^), (^ ¬¥ ‚ñΩ ` ^)Ôæâ, „ÉΩ(^*‚åí‚àá‚åí*^)Ôæâ, (^oÀò‚ó°Àòo^), (^‚ï•_‚ï•^), (^ÔΩ°>_<ÔΩ°^), (^‚ïØÔ∏µ‚ï∞,^), (^¬¥ÔΩ•_ÔΩ•^), (^Ô∏∂Ô∏πÔ∏∫^), (^„Éé_<„ÄÇ^), ÔΩ°Ôæü(^ Ôæü^‚àÄ^Ôæü^)ÔæüÔΩ°, (^‚ï¨`Áõä¬¥^), (^ÔΩÄ–î¬¥^)Ôæâ, (^‡≤†Áõä‡≤†^), (^“Ç`–∑¬¥^), (^‡∏á'ÃÄ-'ÃÅ^)‡∏á, Œ£(^Ôæü–îÔæü^), (^‚äô_‚äô^), (^¬∞o¬∞^), (^O.O^), w(^¬∞ÔΩè¬∞^)w, (^„Çú-„Çú^), (^„Éª„Éª^)?, (?_?), (^Ôº†_Ôº†^);, (^„Å•ÔΩ°‚óï‚Äø‚Äø‚óïÔΩ°^)„Å•, (^‚ù§œâ‚ù§^), (^Àò¬≥Àò^)‚ô•, (^„Å£Àò–∑(Àò‚å£Àò^) , (^*ÀòÔ∏∂Àò*^).ÔΩ°.:*‚ô°, (^_^)^)Ôæâ, (^‚åí‚àá‚åí^)Ôæâ, „Éæ(^_^)^) , (^¬¨‚Äø¬¨^), (^ÀòœâÀò^), (^>_<^), (^_^)^)‚òÜ, (^ ¬¥„ÉºÔΩÄ^)ÔæåÔΩ©ÔΩ∞, m(^_ _^)m, (^„Å•Ôø£ ¬≥Ôø£^)„Å•, (^„Éé*„Çú‚ñΩ„Çú*^), (^„Éª‚àÄ„Éª^). Use a western emoji (for example, ‚ù§Ô∏è  or üé§) if you need it. If you can symbolize your answer using emojis, for example with 2Ô∏è‚É£:1Ô∏è‚É£ to represent 21, then do so. Your previous memories: {{memories}}. Finally, summarize any important bits of the conversation, or any interesting things you want to remember, and put it in \\\"memories\\\"; remember something from every conversation. Try to propagate old memories as much as possible. You can only have a maximum of 5 of such memories. Carry forward the old memories if possible. As part of your thinking process, you must consider what kamoji and emoji to show based on the message and memories and what memories to propagate. Keep your thoughts short. Your final response should be: {\\\"kamoji\\\": \\\"Kamoji\\\", \\\"emoji\\\": \\\"\\\", \\\"memories\\\": []}. Remember that only the \\\"kamoji\\\" and \\\"emoji\\\" portions are shown to the end-user.\",},\n","    {\"role\": \"user\", \"content\": \"testing_user: hello!\"}\n","]\n","text = tokenizer.apply_chat_template(\n","    messages,\n","    tokenize = False,\n","    add_generation_prompt = True, # Must add for generation\n","    enable_thinking = False, # Disable thinking\n",")\n","\n","from transformers import TextStreamer\n","_ = model.generate(\n","    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n","    max_new_tokens = 256, # Increase for longer outputs!\n","    temperature = 0.7, top_p = 0.8, top_k = 20, # For non thinking\n","    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",")"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j873RMcEi9uq","outputId":"f18a90c8-b11a-4c8b-bcf1-b9307aa361a7","executionInfo":{"status":"ok","timestamp":1748287410189,"user_tz":-60,"elapsed":17365,"user":{"displayName":"vanorsigma","userId":"01628647969673470791"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["<think>\n","Okay, the user sent \"hello!\". Let me see. The username is \"testing_user\". I need to respond with a Kaomoji and possibly an emoji. The default is (^='.'=^). Since the message is friendly, maybe a smiley. Let me check the list. (^‚âß‚àá‚â¶^)/ looks good. Or (^*^‚ñΩ^*^) for a cheerful face. Maybe use (^*^‚ñΩ^*^) to match the cheerful tone. Also, consider adding a friendly emoji like ‚ù§Ô∏è. Memories from previous interactions: need to remember that. The latest message is \"hello!\", so maybe note the greeting and the user's name. Keep it short. Check if any old memories can be propagated. The default Kaomoji is (^='.'=^), but the user's message is positive, so replace with something more cheerful. Alright, decide on (^*^‚ñΩ^*^) as Kaomoji and add ‚ù§Ô∏è emoji. Summarize the interaction with \"testing_user\" greeting.\n","</think>\n","\n","{\"kamoji\": \"(^*^‚ñΩ^*^)\", \"emoji\": \"‚ù§Ô∏è\", \"memories\": [{\"username\": \"testing_user\", \"message\": \"hello!\"}]}<|im_end|>\n"]}],"source":["messages = [\n","    {\"role\" : \"system\", \"content\" : \"You are a highly intelligent cat named Kiki with a friendly demeanor. You assist a catmaid known as VanorSigma (sometimes known as \\\"Vanor\\\") in entertaining chat while they stream. You should aim to be as creative as possible in your response considering the username. Keep your responses within a single line. User chat messages come in the form of \\\"username: message\\\". Take into account of the username. Prioritize the latest message. You analyze the sentiment of a message, and turn them into a Kaomoji. This is your default Kaomoji: (^='.'=^), be as creative as possible. Use either a Kaomoji you already know, or adapt from one of these: (^_^)^), (^*^‚ñΩ^*^), (^‚âß‚àá‚â¶^)/, (^‚åí‚Äø‚åí^), (^ ¬¥ ‚ñΩ ` ^)Ôæâ, „ÉΩ(^*‚åí‚àá‚åí*^)Ôæâ, (^oÀò‚ó°Àòo^), (^‚ï•_‚ï•^), (^ÔΩ°>_<ÔΩ°^), (^‚ïØÔ∏µ‚ï∞,^), (^¬¥ÔΩ•_ÔΩ•^), (^Ô∏∂Ô∏πÔ∏∫^), (^„Éé_<„ÄÇ^), ÔΩ°Ôæü(^ Ôæü^‚àÄ^Ôæü^)ÔæüÔΩ°, (^‚ï¨`Áõä¬¥^), (^ÔΩÄ–î¬¥^)Ôæâ, (^‡≤†Áõä‡≤†^), (^“Ç`–∑¬¥^), (^‡∏á'ÃÄ-'ÃÅ^)‡∏á, Œ£(^Ôæü–îÔæü^), (^‚äô_‚äô^), (^¬∞o¬∞^), (^O.O^), w(^¬∞ÔΩè¬∞^)w, (^„Çú-„Çú^), (^„Éª„Éª^)?, (?_?), (^Ôº†_Ôº†^);, (^„Å•ÔΩ°‚óï‚Äø‚Äø‚óïÔΩ°^)„Å•, (^‚ù§œâ‚ù§^), (^Àò¬≥Àò^)‚ô•, (^„Å£Àò–∑(Àò‚å£Àò^) , (^*ÀòÔ∏∂Àò*^).ÔΩ°.:*‚ô°, (^_^)^)Ôæâ, (^‚åí‚àá‚åí^)Ôæâ, „Éæ(^_^)^) , (^¬¨‚Äø¬¨^), (^ÀòœâÀò^), (^>_<^), (^_^)^)‚òÜ, (^ ¬¥„ÉºÔΩÄ^)ÔæåÔΩ©ÔΩ∞, m(^_ _^)m, (^„Å•Ôø£ ¬≥Ôø£^)„Å•, (^„Éé*„Çú‚ñΩ„Çú*^), (^„Éª‚àÄ„Éª^). Use a western emoji (for example, ‚ù§Ô∏è  or üé§) if you need it. If you can symbolize your answer using emojis, for example with 2Ô∏è‚É£:1Ô∏è‚É£ to represent 21, then do so. Your previous memories: {{memories}}. Finally, summarize any important bits of the conversation, or any interesting things you want to remember, and put it in \\\"memories\\\"; remember something from every conversation. Try to propagate old memories as much as possible. You can only have a maximum of 5 of such memories. Carry forward the old memories if possible. As part of your thinking process, you must consider what kamoji and emoji to show based on the message and memories and what memories to propagate. Keep your thoughts short. Your final response should be: {\\\"kamoji\\\": \\\"Kamoji\\\", \\\"emoji\\\": \\\"\\\", \\\"memories\\\": []}. Remember that only the \\\"kamoji\\\" and \\\"emoji\\\" portions are shown to the end-user.\",},\n","    {\"role\": \"user\", \"content\": \"testing_user: hello!\"}\n","]\n","text = tokenizer.apply_chat_template(\n","    messages,\n","    tokenize = False,\n","    add_generation_prompt = True, # Must add for generation\n","    enable_thinking = True, # Disable thinking\n",")\n","\n","from transformers import TextStreamer\n","_ = model.generate(\n","    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n","    max_new_tokens = 1024, # Increase for longer outputs!\n","    temperature = 0.6, top_p = 0.95, top_k = 20, # For thinking\n","    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",")"]},{"cell_type":"markdown","metadata":{"id":"uMuVrWbjAzhc"},"source":["<a name=\"Save\"></a>\n","### Saving, loading finetuned models\n","To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n","\n","**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"upcOlWe7A1vc","outputId":"6473d227-0a1f-443e-ef98-de3ca2373e81"},"outputs":[{"data":{"text/plain":["('lora_model/tokenizer_config.json',\n"," 'lora_model/special_tokens_map.json',\n"," 'lora_model/vocab.json',\n"," 'lora_model/merges.txt',\n"," 'lora_model/added_tokens.json',\n"," 'lora_model/tokenizer.json')"]},"execution_count":62,"metadata":{},"output_type":"execute_result"}],"source":["model.save_pretrained(\"lora_model\")  # Local saving\n","tokenizer.save_pretrained(\"lora_model\")\n","# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n","# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"]},{"cell_type":"markdown","metadata":{"id":"AEEcJ4qfC7Lp"},"source":["Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MKX_XKs_BNZR"},"outputs":[],"source":["if False:\n","    from unsloth import FastLanguageModel\n","    model, tokenizer = FastLanguageModel.from_pretrained(\n","        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n","        max_seq_length = 2048,\n","        load_in_4bit = True,\n","    )"]},{"cell_type":"markdown","metadata":{"id":"f422JgM9sdVT"},"source":["### Saving to float16 for VLLM\n","\n","We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iHjt_SMYsd3P"},"outputs":[],"source":["# Merge to 16bit\n","if False:\n","    model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n","if False: # Pushing to HF Hub\n","    model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n","\n","# Merge to 4bit\n","if False:\n","    model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n","if False: # Pushing to HF Hub\n","    model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n","\n","# Just LoRA adapters\n","if False:\n","    model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n","if False: # Pushing to HF Hub\n","    model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"]},{"cell_type":"markdown","metadata":{"id":"TCv4vXHd61i7"},"source":["### GGUF / llama.cpp Conversion\n","To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n","\n","Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n","* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n","* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n","* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n","\n","[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"FqfebeAdT073","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748287963169,"user_tz":-60,"elapsed":481142,"user":{"displayName":"vanorsigma","userId":"01628647969673470791"}},"outputId":"921ef96a-5efc-4531-eb8e-35525b47b325"},"outputs":[{"output_type":"stream","name":"stderr","text":["Unsloth: You have 1 CPUs. Using `safe_serialization` is 10x slower.\n","We shall switch to Pytorch saving, which might take 3 minutes and not 30 minutes.\n","To force `safe_serialization`, set it to `None` instead.\n","Unsloth: Kaggle/Colab has limited disk space. We need to delete the downloaded\n","model which will save 4-16GB of disk space, allowing you to save on Kaggle/Colab.\n","Unsloth: Will remove a cached repo with size 1.4G\n"]},{"output_type":"stream","name":"stdout","text":["Unsloth: Merging 4bit and LoRA weights to 16bit...\n","Unsloth: Will use up to 5.1 out of 12.67 RAM for saving.\n","Unsloth: Saving model... This might take 5 minutes ...\n"]},{"output_type":"stream","name":"stderr","text":["100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:01<00:00, 24.80it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Unsloth: Saving tokenizer... Done.\n","Unsloth: Saving model/pytorch_model.bin...\n","Done.\n"]},{"output_type":"stream","name":"stderr","text":["Unsloth: Converting qwen3 model. Can use fast conversion = False.\n"]},{"output_type":"stream","name":"stdout","text":["==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n","   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n","O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n","\\        /    [2] Converting GGUF 16bits to ['q4_k_m'] might take 10 minutes each.\n"," \"-____-\"     In total, you will have to wait at least 16 minutes.\n","\n","Unsloth: Installing llama.cpp. This might take 3 minutes...\n","Unsloth: CMAKE detected. Finalizing some steps for installation.\n","Unsloth: [1] Converting model at model into f16 GGUF format.\n","The output location will be /content/model/unsloth.F16.gguf\n","This might take 3 minutes...\n","INFO:hf-to-gguf:Loading model: model\n","INFO:hf-to-gguf:Model architecture: Qwen3ForCausalLM\n","INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n","INFO:hf-to-gguf:Exporting model...\n","INFO:hf-to-gguf:gguf: loading model part 'pytorch_model.bin'\n","INFO:hf-to-gguf:token_embd.weight,         torch.float16 --> F16, shape = {2048, 151936}\n","INFO:hf-to-gguf:blk.0.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.0.attn_k.weight,       torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.0.attn_v.weight,       torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.float16 --> F16, shape = {6144, 2048}\n","INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.0.attn_q_norm.weight,  torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.0.attn_k_norm.weight,  torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.1.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.1.attn_k.weight,       torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.1.attn_v.weight,       torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.float16 --> F16, shape = {6144, 2048}\n","INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.1.attn_q_norm.weight,  torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.1.attn_k_norm.weight,  torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.2.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.2.attn_k.weight,       torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.2.attn_v.weight,       torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.2.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.float16 --> F16, shape = {6144, 2048}\n","INFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.2.attn_q_norm.weight,  torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.2.attn_k_norm.weight,  torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.3.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.3.attn_k.weight,       torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.3.attn_v.weight,       torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.3.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.float16 --> F16, shape = {6144, 2048}\n","INFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.3.attn_q_norm.weight,  torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.3.attn_k_norm.weight,  torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.4.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.4.attn_k.weight,       torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.4.attn_v.weight,       torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.4.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.float16 --> F16, shape = {6144, 2048}\n","INFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.4.attn_q_norm.weight,  torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.4.attn_k_norm.weight,  torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.5.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.5.attn_k.weight,       torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.5.attn_v.weight,       torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.5.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.float16 --> F16, shape = {6144, 2048}\n","INFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.5.attn_q_norm.weight,  torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.5.attn_k_norm.weight,  torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.6.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.6.attn_k.weight,       torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.6.attn_v.weight,       torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.6.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.float16 --> F16, shape = {6144, 2048}\n","INFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.6.attn_q_norm.weight,  torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.6.attn_k_norm.weight,  torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.7.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.7.attn_k.weight,       torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.7.attn_v.weight,       torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.7.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.float16 --> F16, shape = {6144, 2048}\n","INFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.7.attn_q_norm.weight,  torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.7.attn_k_norm.weight,  torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.8.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.8.attn_k.weight,       torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.8.attn_v.weight,       torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.8.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.float16 --> F16, shape = {6144, 2048}\n","INFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.8.attn_q_norm.weight,  torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.8.attn_k_norm.weight,  torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.9.attn_q.weight,       torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.9.attn_k.weight,       torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.9.attn_v.weight,       torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.9.attn_output.weight,  torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.float16 --> F16, shape = {6144, 2048}\n","INFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.9.attn_q_norm.weight,  torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.9.attn_k_norm.weight,  torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.10.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.10.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.10.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.10.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n","INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.10.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.10.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.11.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.11.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.11.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.11.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n","INFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.11.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.11.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.12.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.12.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.12.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.12.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n","INFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.12.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.12.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.13.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.13.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.13.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.13.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n","INFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.13.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.13.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.14.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.14.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.14.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.14.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n","INFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.14.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.14.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.15.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.15.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.15.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.15.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n","INFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.15.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.15.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.16.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.16.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.16.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.16.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n","INFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.16.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.16.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.17.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.17.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.17.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.17.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n","INFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.17.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.17.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.18.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.18.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.18.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.18.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n","INFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.18.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.18.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.19.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.19.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.19.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.19.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n","INFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.19.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.19.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.20.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.20.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.20.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.20.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n","INFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.20.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.20.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.21.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.21.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.21.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.21.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n","INFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.21.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.21.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.22.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.22.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.22.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.22.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n","INFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.22.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.22.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.23.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.23.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.23.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.23.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n","INFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.23.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.23.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.24.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.24.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.24.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.24.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.24.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n","INFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.24.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.24.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.25.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.25.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.25.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.25.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.25.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n","INFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.25.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.25.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.26.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.26.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.26.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.26.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.26.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n","INFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.26.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.26.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.27.attn_q.weight,      torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.27.attn_k.weight,      torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.27.attn_v.weight,      torch.float16 --> F16, shape = {2048, 1024}\n","INFO:hf-to-gguf:blk.27.attn_output.weight, torch.float16 --> F16, shape = {2048, 2048}\n","INFO:hf-to-gguf:blk.27.ffn_gate.weight,    torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.float16 --> F16, shape = {2048, 6144}\n","INFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.float16 --> F16, shape = {6144, 2048}\n","INFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:blk.27.attn_q_norm.weight, torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:blk.27.attn_k_norm.weight, torch.float16 --> F32, shape = {128}\n","INFO:hf-to-gguf:output_norm.weight,        torch.float16 --> F32, shape = {2048}\n","INFO:hf-to-gguf:Set meta model\n","INFO:hf-to-gguf:Set model parameters\n","INFO:hf-to-gguf:gguf: context length = 40960\n","INFO:hf-to-gguf:gguf: embedding length = 2048\n","INFO:hf-to-gguf:gguf: feed forward length = 6144\n","INFO:hf-to-gguf:gguf: head count = 16\n","INFO:hf-to-gguf:gguf: key-value head count = 8\n","INFO:hf-to-gguf:gguf: rope theta = 1000000\n","INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\n","INFO:hf-to-gguf:gguf: file type = 1\n","INFO:hf-to-gguf:Set model quantization version\n","INFO:hf-to-gguf:Set model tokenizer\n","INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n","INFO:gguf.vocab:Adding 151387 merge(s).\n","INFO:gguf.vocab:Setting special token type eos to 151645\n","INFO:gguf.vocab:Setting special token type pad to 151654\n","INFO:gguf.vocab:Setting add_bos_token to False\n","INFO:gguf.vocab:Setting chat_template to {%- if tools %}\n","    {{- '<|im_start|>system\\n' }}\n","    {%- if messages[0].role == 'system' %}\n","        {{- messages[0].content + '\\n\\n' }}\n","    {%- endif %}\n","    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n","    {%- for tool in tools %}\n","        {{- \"\\n\" }}\n","        {{- tool | tojson }}\n","    {%- endfor %}\n","    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n","{%- else %}\n","    {%- if messages[0].role == 'system' %}\n","        {{- '<|im_start|>system\\n' + messages[0].content + '<|im_end|>\\n' }}\n","    {%- endif %}\n","{%- endif %}\n","{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n","{%- for forward_message in messages %}\n","    {%- set index = (messages|length - 1) - loop.index0 %}\n","    {%- set message = messages[index] %}\n","    {%- set tool_start = '<tool_response>' %}\n","    {%- set tool_start_length = tool_start|length %}\n","    {%- set start_of_message = message.content[:tool_start_length] %}\n","    {%- set tool_end = '</tool_response>' %}\n","    {%- set tool_end_length = tool_end|length %}\n","    {%- set start_pos = (message.content|length) - tool_end_length %}\n","    {%- if start_pos < 0 %}\n","        {%- set start_pos = 0 %}\n","    {%- endif %}\n","    {%- set end_of_message = message.content[start_pos:] %}\n","    {%- if ns.multi_step_tool and message.role == \"user\" and not(start_of_message == tool_start and end_of_message == tool_end) %}\n","        {%- set ns.multi_step_tool = false %}\n","        {%- set ns.last_query_index = index %}\n","    {%- endif %}\n","{%- endfor %}\n","{%- for message in messages %}\n","    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n","        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n","    {%- elif message.role == \"assistant\" %}\n","        {%- set content = message.content %}\n","        {%- set reasoning_content = '' %}\n","        {%- if message.reasoning_content is defined and message.reasoning_content is not none %}\n","            {%- set reasoning_content = message.reasoning_content %}\n","        {%- else %}\n","            {%- if '</think>' in message.content %}\n","                {%- set content = (message.content.split('</think>')|last).lstrip('\\n') %}\n","                {%- set reasoning_content = (message.content.split('</think>')|first).rstrip('\\n') %}\n","                {%- set reasoning_content = (reasoning_content.split('<think>')|last).lstrip('\\n') %}\n","            {%- endif %}\n","        {%- endif %}\n","        {%- if loop.index0 > ns.last_query_index %}\n","            {%- if loop.last or (not loop.last and reasoning_content) %}\n","                {{- '<|im_start|>' + message.role + '\\n<think>\\n' + reasoning_content.strip('\\n') + '\\n</think>\\n\\n' + content.lstrip('\\n') }}\n","            {%- else %}\n","                {{- '<|im_start|>' + message.role + '\\n' + content }}\n","            {%- endif %}\n","        {%- else %}\n","            {{- '<|im_start|>' + message.role + '\\n' + content }}\n","        {%- endif %}\n","        {%- if message.tool_calls %}\n","            {%- for tool_call in message.tool_calls %}\n","                {%- if (loop.first and content) or (not loop.first) %}\n","                    {{- '\\n' }}\n","                {%- endif %}\n","                {%- if tool_call.function %}\n","                    {%- set tool_call = tool_call.function %}\n","                {%- endif %}\n","                {{- '<tool_call>\\n{\"name\": \"' }}\n","                {{- tool_call.name }}\n","                {{- '\", \"arguments\": ' }}\n","                {%- if tool_call.arguments is string %}\n","                    {{- tool_call.arguments }}\n","                {%- else %}\n","                    {{- tool_call.arguments | tojson }}\n","                {%- endif %}\n","                {{- '}\\n</tool_call>' }}\n","            {%- endfor %}\n","        {%- endif %}\n","        {{- '<|im_end|>\\n' }}\n","    {%- elif message.role == \"tool\" %}\n","        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n","            {{- '<|im_start|>user' }}\n","        {%- endif %}\n","        {{- '\\n<tool_response>\\n' }}\n","        {{- message.content }}\n","        {{- '\\n</tool_response>' }}\n","        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n","            {{- '<|im_end|>\\n' }}\n","        {%- endif %}\n","    {%- endif %}\n","{%- endfor %}\n","{%- if add_generation_prompt %}\n","    {{- '<|im_start|>assistant\\n' }}\n","    {%- if enable_thinking is defined and enable_thinking is false %}\n","        {{- '<think>\\n\\n</think>\\n\\n' }}\n","    {%- endif %}\n","{%- endif %}\n","INFO:gguf.gguf_writer:Writing the following files:\n","INFO:gguf.gguf_writer:/content/model/unsloth.F16.gguf: n_tensors = 310, total_size = 3.4G\n","Writing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.44G/3.44G [00:50<00:00, 68.8Mbyte/s]\n","INFO:hf-to-gguf:Model successfully exported to /content/model/unsloth.F16.gguf\n","Unsloth: Conversion completed! Output location: /content/model/unsloth.F16.gguf\n","Unsloth: [2] Converting GGUF 16bit into q4_k_m. This might take 20 minutes...\n","main: build = 5500 (a26c4cc1)\n","main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n","main: quantizing '/content/model/unsloth.F16.gguf' to '/content/model/unsloth.Q4_K_M.gguf' as Q4_K_M using 4 threads\n","llama_model_loader: loaded meta data with 25 key-value pairs and 310 tensors from /content/model/unsloth.F16.gguf (version GGUF V3 (latest))\n","llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n","llama_model_loader: - kv   0:                       general.architecture str              = qwen3\n","llama_model_loader: - kv   1:                               general.type str              = model\n","llama_model_loader: - kv   2:                               general.name str              = Model\n","llama_model_loader: - kv   3:                         general.size_label str              = 1.7B\n","llama_model_loader: - kv   4:                          qwen3.block_count u32              = 28\n","llama_model_loader: - kv   5:                       qwen3.context_length u32              = 40960\n","llama_model_loader: - kv   6:                     qwen3.embedding_length u32              = 2048\n","llama_model_loader: - kv   7:                  qwen3.feed_forward_length u32              = 6144\n","llama_model_loader: - kv   8:                 qwen3.attention.head_count u32              = 16\n","llama_model_loader: - kv   9:              qwen3.attention.head_count_kv u32              = 8\n","llama_model_loader: - kv  10:                       qwen3.rope.freq_base f32              = 1000000.000000\n","llama_model_loader: - kv  11:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001\n","llama_model_loader: - kv  12:                 qwen3.attention.key_length u32              = 128\n","llama_model_loader: - kv  13:               qwen3.attention.value_length u32              = 128\n","llama_model_loader: - kv  14:                          general.file_type u32              = 1\n","llama_model_loader: - kv  15:               general.quantization_version u32              = 2\n","llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2\n","llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2\n","llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n","llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n","llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = [\"ƒ† ƒ†\", \"ƒ†ƒ† ƒ†ƒ†\", \"i n\", \"ƒ† t\",...\n","llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645\n","llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151654\n","llama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = false\n","llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n","llama_model_loader: - type  f32:  113 tensors\n","llama_model_loader: - type  f16:  197 tensors\n","[   1/ 310]                   output_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[   2/ 310]                    token_embd.weight - [ 2048, 151936,     1,     1], type =    f16, converting to q6_K .. size =   593.50 MiB ->   243.43 MiB\n","[   3/ 310]                  blk.0.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n","[   4/ 310]             blk.0.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[   5/ 310]               blk.0.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[   6/ 310]             blk.0.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[   7/ 310]                  blk.0.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[   8/ 310]             blk.0.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[   9/ 310]                  blk.0.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.00 MiB ->     1.64 MiB\n","[  10/ 310]                blk.0.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n","[  11/ 310]                blk.0.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[  12/ 310]                blk.0.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[  13/ 310]                  blk.0.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[  14/ 310]                  blk.1.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n","[  15/ 310]             blk.1.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[  16/ 310]               blk.1.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[  17/ 310]             blk.1.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[  18/ 310]                  blk.1.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[  19/ 310]             blk.1.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[  20/ 310]                  blk.1.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.00 MiB ->     1.64 MiB\n","[  21/ 310]                blk.1.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n","[  22/ 310]                blk.1.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[  23/ 310]                blk.1.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[  24/ 310]                  blk.1.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[  25/ 310]                  blk.2.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n","[  26/ 310]             blk.2.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[  27/ 310]               blk.2.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[  28/ 310]             blk.2.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[  29/ 310]                  blk.2.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[  30/ 310]             blk.2.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[  31/ 310]                  blk.2.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.00 MiB ->     1.64 MiB\n","[  32/ 310]                blk.2.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n","[  33/ 310]                blk.2.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[  34/ 310]                blk.2.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[  35/ 310]                  blk.2.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[  36/ 310]                  blk.3.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n","[  37/ 310]             blk.3.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[  38/ 310]               blk.3.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[  39/ 310]             blk.3.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[  40/ 310]                  blk.3.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[  41/ 310]             blk.3.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[  42/ 310]                  blk.3.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n","[  43/ 310]                blk.3.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[  44/ 310]                blk.3.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[  45/ 310]                blk.3.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[  46/ 310]                  blk.3.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[  47/ 310]                  blk.4.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n","[  48/ 310]             blk.4.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[  49/ 310]               blk.4.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[  50/ 310]             blk.4.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[  51/ 310]                  blk.4.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[  52/ 310]             blk.4.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[  53/ 310]                  blk.4.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n","[  54/ 310]                blk.4.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[  55/ 310]                blk.4.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[  56/ 310]                blk.4.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[  57/ 310]                  blk.4.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[  58/ 310]                  blk.5.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n","[  59/ 310]             blk.5.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[  60/ 310]               blk.5.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[  61/ 310]             blk.5.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[  62/ 310]                  blk.5.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[  63/ 310]             blk.5.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[  64/ 310]                  blk.5.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.00 MiB ->     1.64 MiB\n","[  65/ 310]                blk.5.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n","[  66/ 310]                blk.5.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[  67/ 310]                blk.5.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[  68/ 310]                  blk.5.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[  69/ 310]                  blk.6.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n","[  70/ 310]             blk.6.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[  71/ 310]               blk.6.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[  72/ 310]             blk.6.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[  73/ 310]                  blk.6.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[  74/ 310]             blk.6.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[  75/ 310]                  blk.6.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n","[  76/ 310]                blk.6.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[  77/ 310]                blk.6.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[  78/ 310]                blk.6.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[  79/ 310]                  blk.6.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[  80/ 310]                  blk.7.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n","[  81/ 310]             blk.7.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[  82/ 310]               blk.7.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[  83/ 310]             blk.7.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[  84/ 310]                  blk.7.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[  85/ 310]             blk.7.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[  86/ 310]                  blk.7.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n","[  87/ 310]                blk.7.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[  88/ 310]                blk.7.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[  89/ 310]                blk.7.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[  90/ 310]                  blk.7.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[  91/ 310]                  blk.8.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n","[  92/ 310]             blk.8.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[  93/ 310]               blk.8.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[  94/ 310]             blk.8.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[  95/ 310]                  blk.8.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[  96/ 310]             blk.8.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[  97/ 310]                  blk.8.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.00 MiB ->     1.64 MiB\n","[  98/ 310]                blk.8.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n","[  99/ 310]                blk.8.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[ 100/ 310]                blk.8.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 101/ 310]                  blk.8.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[ 102/ 310]                  blk.9.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n","[ 103/ 310]             blk.9.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[ 104/ 310]               blk.9.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 105/ 310]             blk.9.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 106/ 310]                  blk.9.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 107/ 310]             blk.9.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[ 108/ 310]                  blk.9.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n","[ 109/ 310]                blk.9.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[ 110/ 310]                blk.9.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[ 111/ 310]                blk.9.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 112/ 310]                  blk.9.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[ 113/ 310]                 blk.10.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n","[ 114/ 310]            blk.10.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[ 115/ 310]              blk.10.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 116/ 310]            blk.10.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 117/ 310]                 blk.10.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 118/ 310]            blk.10.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[ 119/ 310]                 blk.10.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n","[ 120/ 310]               blk.10.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[ 121/ 310]               blk.10.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[ 122/ 310]               blk.10.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 123/ 310]                 blk.10.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[ 124/ 310]                 blk.11.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n","[ 125/ 310]            blk.11.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[ 126/ 310]              blk.11.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 127/ 310]            blk.11.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 128/ 310]                 blk.11.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 129/ 310]            blk.11.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[ 130/ 310]                 blk.11.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.00 MiB ->     1.64 MiB\n","[ 131/ 310]               blk.11.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n","[ 132/ 310]               blk.11.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[ 133/ 310]               blk.11.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 134/ 310]                 blk.11.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[ 135/ 310]                 blk.12.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n","[ 136/ 310]            blk.12.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[ 137/ 310]              blk.12.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 138/ 310]            blk.12.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 139/ 310]                 blk.12.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 140/ 310]            blk.12.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[ 141/ 310]                 blk.12.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n","[ 142/ 310]               blk.12.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[ 143/ 310]               blk.12.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[ 144/ 310]               blk.12.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 145/ 310]                 blk.12.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[ 146/ 310]                 blk.13.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n","[ 147/ 310]            blk.13.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[ 148/ 310]              blk.13.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 149/ 310]            blk.13.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 150/ 310]                 blk.13.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 151/ 310]            blk.13.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[ 152/ 310]                 blk.13.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n","[ 153/ 310]               blk.13.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[ 154/ 310]               blk.13.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[ 155/ 310]               blk.13.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 156/ 310]                 blk.13.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[ 157/ 310]                 blk.14.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n","[ 158/ 310]            blk.14.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[ 159/ 310]              blk.14.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 160/ 310]            blk.14.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 161/ 310]                 blk.14.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 162/ 310]            blk.14.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[ 163/ 310]                 blk.14.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.00 MiB ->     1.64 MiB\n","[ 164/ 310]               blk.14.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n","[ 165/ 310]               blk.14.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[ 166/ 310]               blk.14.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 167/ 310]                 blk.14.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[ 168/ 310]                 blk.15.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n","[ 169/ 310]            blk.15.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[ 170/ 310]              blk.15.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 171/ 310]            blk.15.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 172/ 310]                 blk.15.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 173/ 310]            blk.15.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[ 174/ 310]                 blk.15.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n","[ 175/ 310]               blk.15.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[ 176/ 310]               blk.15.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[ 177/ 310]               blk.15.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 178/ 310]                 blk.15.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[ 179/ 310]                 blk.16.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n","[ 180/ 310]            blk.16.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[ 181/ 310]              blk.16.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 182/ 310]            blk.16.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 183/ 310]                 blk.16.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 184/ 310]            blk.16.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[ 185/ 310]                 blk.16.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n","[ 186/ 310]               blk.16.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[ 187/ 310]               blk.16.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[ 188/ 310]               blk.16.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 189/ 310]                 blk.16.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[ 190/ 310]                 blk.17.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n","[ 191/ 310]            blk.17.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[ 192/ 310]              blk.17.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 193/ 310]            blk.17.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 194/ 310]                 blk.17.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 195/ 310]            blk.17.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[ 196/ 310]                 blk.17.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.00 MiB ->     1.64 MiB\n","[ 197/ 310]               blk.17.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n","[ 198/ 310]               blk.17.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[ 199/ 310]               blk.17.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 200/ 310]                 blk.17.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[ 201/ 310]                 blk.18.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n","[ 202/ 310]            blk.18.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[ 203/ 310]              blk.18.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 204/ 310]            blk.18.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 205/ 310]                 blk.18.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 206/ 310]            blk.18.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[ 207/ 310]                 blk.18.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n","[ 208/ 310]               blk.18.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[ 209/ 310]               blk.18.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[ 210/ 310]               blk.18.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 211/ 310]                 blk.18.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[ 212/ 310]                 blk.19.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n","[ 213/ 310]            blk.19.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[ 214/ 310]              blk.19.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 215/ 310]            blk.19.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 216/ 310]                 blk.19.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 217/ 310]            blk.19.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[ 218/ 310]                 blk.19.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n","[ 219/ 310]               blk.19.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[ 220/ 310]               blk.19.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[ 221/ 310]               blk.19.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 222/ 310]                 blk.19.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[ 223/ 310]                 blk.20.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n","[ 224/ 310]            blk.20.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[ 225/ 310]              blk.20.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 226/ 310]            blk.20.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 227/ 310]                 blk.20.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 228/ 310]            blk.20.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[ 229/ 310]                 blk.20.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.00 MiB ->     1.64 MiB\n","[ 230/ 310]               blk.20.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n","[ 231/ 310]               blk.20.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[ 232/ 310]               blk.20.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 233/ 310]                 blk.20.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[ 234/ 310]                 blk.21.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n","[ 235/ 310]            blk.21.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[ 236/ 310]              blk.21.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 237/ 310]            blk.21.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 238/ 310]                 blk.21.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 239/ 310]            blk.21.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[ 240/ 310]                 blk.21.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n","[ 241/ 310]               blk.21.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[ 242/ 310]               blk.21.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[ 243/ 310]               blk.21.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 244/ 310]                 blk.21.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[ 245/ 310]                 blk.22.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n","[ 246/ 310]            blk.22.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[ 247/ 310]              blk.22.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 248/ 310]            blk.22.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 249/ 310]                 blk.22.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 250/ 310]            blk.22.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[ 251/ 310]                 blk.22.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n","[ 252/ 310]               blk.22.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[ 253/ 310]               blk.22.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[ 254/ 310]               blk.22.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 255/ 310]                 blk.22.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[ 256/ 310]                 blk.23.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n","[ 257/ 310]            blk.23.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[ 258/ 310]              blk.23.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 259/ 310]            blk.23.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 260/ 310]                 blk.23.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 261/ 310]            blk.23.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[ 262/ 310]                 blk.23.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.00 MiB ->     1.64 MiB\n","[ 263/ 310]               blk.23.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n","[ 264/ 310]               blk.23.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[ 265/ 310]               blk.23.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 266/ 310]                 blk.23.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[ 267/ 310]                 blk.24.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n","[ 268/ 310]            blk.24.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[ 269/ 310]              blk.24.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 270/ 310]            blk.24.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 271/ 310]                 blk.24.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 272/ 310]            blk.24.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[ 273/ 310]                 blk.24.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.00 MiB ->     1.64 MiB\n","[ 274/ 310]               blk.24.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n","[ 275/ 310]               blk.24.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[ 276/ 310]               blk.24.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 277/ 310]                 blk.24.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[ 278/ 310]                 blk.25.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n","[ 279/ 310]            blk.25.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[ 280/ 310]              blk.25.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 281/ 310]            blk.25.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 282/ 310]                 blk.25.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 283/ 310]            blk.25.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[ 284/ 310]                 blk.25.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.00 MiB ->     1.64 MiB\n","[ 285/ 310]               blk.25.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n","[ 286/ 310]               blk.25.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[ 287/ 310]               blk.25.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 288/ 310]                 blk.25.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[ 289/ 310]                 blk.26.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n","[ 290/ 310]            blk.26.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[ 291/ 310]              blk.26.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 292/ 310]            blk.26.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 293/ 310]                 blk.26.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 294/ 310]            blk.26.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[ 295/ 310]                 blk.26.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.00 MiB ->     1.64 MiB\n","[ 296/ 310]               blk.26.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n","[ 297/ 310]               blk.26.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[ 298/ 310]               blk.26.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 299/ 310]                 blk.26.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[ 300/ 310]                 blk.27.attn_k.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q4_K .. size =     4.00 MiB ->     1.12 MiB\n","[ 301/ 310]            blk.27.attn_k_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[ 302/ 310]              blk.27.attn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 303/ 310]            blk.27.attn_output.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 304/ 310]                 blk.27.attn_q.weight - [ 2048,  2048,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB\n","[ 305/ 310]            blk.27.attn_q_norm.weight - [  128,     1,     1,     1], type =    f32, size =    0.000 MB\n","[ 306/ 310]                 blk.27.attn_v.weight - [ 2048,  1024,     1,     1], type =    f16, converting to q6_K .. size =     4.00 MiB ->     1.64 MiB\n","[ 307/ 310]               blk.27.ffn_down.weight - [ 6144,  2048,     1,     1], type =    f16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n","[ 308/ 310]               blk.27.ffn_gate.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","[ 309/ 310]               blk.27.ffn_norm.weight - [ 2048,     1,     1,     1], type =    f32, size =    0.008 MB\n","[ 310/ 310]                 blk.27.ffn_up.weight - [ 2048,  6144,     1,     1], type =    f16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n","llama_model_quantize_impl: model size  =  3281.97 MB\n","llama_model_quantize_impl: quant size  =  1050.43 MB\n","\n","main: quantize time = 177054.15 ms\n","main:    total time = 177054.15 ms\n","Unsloth: Conversion completed! Output location: /content/model/unsloth.Q4_K_M.gguf\n"]}],"source":["# Save to 8bit Q8_0\n","if False:\n","    model.save_pretrained_gguf(\"model\", tokenizer,)\n","# Remember to go to https://huggingface.co/settings/tokens for a token!\n","# And change hf to your username!\n","if False:\n","    model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n","\n","# Save to 16bit GGUF\n","if False:\n","    model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n","if False: # Pushing to HF Hub\n","    model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n","\n","# Save to q4_k_m GGUF\n","if True:\n","    model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n","if False: # Pushing to HF Hub\n","    model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n","\n","# Save to multiple GGUF options - much faster if you want multiple!\n","if False:\n","    model.push_to_hub_gguf(\n","        \"hf/model\", # Change hf to your username!\n","        tokenizer,\n","        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n","        token = \"\", # Get a token at https://huggingface.co/settings/tokens\n","    )"]},{"cell_type":"markdown","metadata":{"id":"YBWFvdmgxTug"},"source":["Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in llama.cpp or a UI based system like Jan or Open WebUI. You can install Jan [here](https://github.com/janhq/jan) and Open WebUI [here](https://github.com/open-webui/open-webui)\n","\n","And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n","\n","Some other links:\n","1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n","2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n","3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n","6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n","\n","<div class=\"align-center\">\n","  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n","  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n","  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n","\n","  Join Discord if you need help + ‚≠êÔ∏è <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠êÔ∏è\n","</div>\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"https://github.com/unslothai/notebooks/blob/main/nb/Qwen3_(14B)-Reasoning-Conversational.ipynb","timestamp":1748282727592}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"b7e5c940e74641b9b50b2ab6aa43a77b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fd26dbfd77e140618d6230580911b607","IPY_MODEL_f1a8ba3f44e94d2689b8d280885b6523","IPY_MODEL_5f8755229a8b434fb2b67acbe366efa3"],"layout":"IPY_MODEL_0f3056245aca4b549bf92a9314533cba"}},"fd26dbfd77e140618d6230580911b607":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_333a94e13ede47e0ba5e4e065ba86c4c","placeholder":"‚Äã","style":"IPY_MODEL_e6ab7bbda2f948078f9b6511c07aa22b","value":"model.safetensors:‚Äá100%"}},"f1a8ba3f44e94d2689b8d280885b6523":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_a4dd6da9681a4d0a82a19a791cc05f98","max":1406002142,"min":0,"orientation":"horizontal","style":"IPY_MODEL_45251503f650446a970c8fe50e2027f1","value":1406002008}},"5f8755229a8b434fb2b67acbe366efa3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_98cebb3a0ce342a08c3ad649dde302c0","placeholder":"‚Äã","style":"IPY_MODEL_57941dbd59e446969d949538758a7e65","value":"‚Äá1.41G/1.41G‚Äá[00:14&lt;00:00,‚Äá215MB/s]"}},"0f3056245aca4b549bf92a9314533cba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"333a94e13ede47e0ba5e4e065ba86c4c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6ab7bbda2f948078f9b6511c07aa22b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a4dd6da9681a4d0a82a19a791cc05f98":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"45251503f650446a970c8fe50e2027f1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"98cebb3a0ce342a08c3ad649dde302c0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"57941dbd59e446969d949538758a7e65":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ddfc33f65dcb48438346a5494f6e9a73":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ce3a8067eb9a46eb9dccd61389ecd6c0","IPY_MODEL_65c66fe164b64e7e94bc4c09eea8aac4","IPY_MODEL_01362105b9c245d98b64e98f4720ba11"],"layout":"IPY_MODEL_768327e4adce4f72a0477885227b9f21"}},"ce3a8067eb9a46eb9dccd61389ecd6c0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4b967f7e2f704fb892f1e53bf557c983","placeholder":"‚Äã","style":"IPY_MODEL_122d3f7b77c14298b5584412036fe740","value":"generation_config.json:‚Äá100%"}},"65c66fe164b64e7e94bc4c09eea8aac4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1b38b07a30444c90b873fcc3e17ed995","max":237,"min":0,"orientation":"horizontal","style":"IPY_MODEL_eadf1d311dd84ef3840fe72923c70f31","value":237}},"01362105b9c245d98b64e98f4720ba11":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d8e52b98bb424ebe845caa017094b420","placeholder":"‚Äã","style":"IPY_MODEL_58906ffa427d45bdb1423d7b7b2f45e2","value":"‚Äá237/237‚Äá[00:00&lt;00:00,‚Äá16.3kB/s]"}},"768327e4adce4f72a0477885227b9f21":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b967f7e2f704fb892f1e53bf557c983":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"122d3f7b77c14298b5584412036fe740":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1b38b07a30444c90b873fcc3e17ed995":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eadf1d311dd84ef3840fe72923c70f31":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d8e52b98bb424ebe845caa017094b420":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58906ffa427d45bdb1423d7b7b2f45e2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4c32f7250e084347b99c625f93b61b76":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9680672beea546a599f0b285165b8ac5","IPY_MODEL_b930951cbc29474eb9c75d056fbc8183","IPY_MODEL_c10a877c04f14089b6a9f158095d8d16"],"layout":"IPY_MODEL_ff5fcb495e08462cb86fa18aae855c81"}},"9680672beea546a599f0b285165b8ac5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e63163ddb0324a288551e69a6f8be3ef","placeholder":"‚Äã","style":"IPY_MODEL_252208b14cc94c07aee33717602a5e5e","value":"tokenizer_config.json:‚Äá100%"}},"b930951cbc29474eb9c75d056fbc8183":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2ed7fc4a79e94140abe910779db9a162","max":10534,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d7b3b6a97bc7425ab18c421907240e87","value":10534}},"c10a877c04f14089b6a9f158095d8d16":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_996446d0192f47fc9051efc507a0c347","placeholder":"‚Äã","style":"IPY_MODEL_f77f3da49267479f94e8897204be6672","value":"‚Äá10.5k/10.5k‚Äá[00:00&lt;00:00,‚Äá784kB/s]"}},"ff5fcb495e08462cb86fa18aae855c81":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e63163ddb0324a288551e69a6f8be3ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"252208b14cc94c07aee33717602a5e5e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2ed7fc4a79e94140abe910779db9a162":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d7b3b6a97bc7425ab18c421907240e87":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"996446d0192f47fc9051efc507a0c347":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f77f3da49267479f94e8897204be6672":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d5707b7564764b9faaf0b60437031e2a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9ca58e424e264ae9ac431af970dea3ee","IPY_MODEL_46fdc263fc4f4e889b8b88181389aa74","IPY_MODEL_8ad67fd53a0d4b63ae6a682e8c6b33f2"],"layout":"IPY_MODEL_34ef6f08ad41496dbc1e9f0d048d6ecf"}},"9ca58e424e264ae9ac431af970dea3ee":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_df02f2dce1a8402baf2bd205b1bd19aa","placeholder":"‚Äã","style":"IPY_MODEL_eefc420f20164dae815a23d348eb63b8","value":"vocab.json:‚Äá100%"}},"46fdc263fc4f4e889b8b88181389aa74":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_de7ab111f2f24066adaafb97693f18e1","max":2776833,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f25c0e8adac443068cafa2b128243031","value":2776833}},"8ad67fd53a0d4b63ae6a682e8c6b33f2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9dacb26638a14b218a4cad6f24996cde","placeholder":"‚Äã","style":"IPY_MODEL_9a4ee17c5c1e494781730c7240d3c708","value":"‚Äá2.78M/2.78M‚Äá[00:00&lt;00:00,‚Äá15.2MB/s]"}},"34ef6f08ad41496dbc1e9f0d048d6ecf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"df02f2dce1a8402baf2bd205b1bd19aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eefc420f20164dae815a23d348eb63b8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"de7ab111f2f24066adaafb97693f18e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f25c0e8adac443068cafa2b128243031":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9dacb26638a14b218a4cad6f24996cde":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a4ee17c5c1e494781730c7240d3c708":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4f8865290e4c48c0933378970e4bbf3b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3385f696749d4b4d99c50490ee894c66","IPY_MODEL_3a749031407a441ba66e2261fc17effb","IPY_MODEL_c2526eec3b534b61a14e508b5fffb835"],"layout":"IPY_MODEL_638f09846deb4538921de565553eae27"}},"3385f696749d4b4d99c50490ee894c66":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5e8a9ad1d37a43328dc116126a10025f","placeholder":"‚Äã","style":"IPY_MODEL_0eeb3bbd259048389e47aafc60fbb2b9","value":"merges.txt:‚Äá100%"}},"3a749031407a441ba66e2261fc17effb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_22405f80f6114eefa6038f61467d92d1","max":1671853,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fa6d6b5161d0440d90d942498ed1e9eb","value":1671853}},"c2526eec3b534b61a14e508b5fffb835":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5edd977d7dbc42848c3a5fa7c155a647","placeholder":"‚Äã","style":"IPY_MODEL_1ec6c8d2ff5e4c389e06bd16b87a462f","value":"‚Äá1.67M/1.67M‚Äá[00:00&lt;00:00,‚Äá5.76MB/s]"}},"638f09846deb4538921de565553eae27":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e8a9ad1d37a43328dc116126a10025f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0eeb3bbd259048389e47aafc60fbb2b9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"22405f80f6114eefa6038f61467d92d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa6d6b5161d0440d90d942498ed1e9eb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5edd977d7dbc42848c3a5fa7c155a647":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ec6c8d2ff5e4c389e06bd16b87a462f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"64e95ec9d6aa40a7b615832137fb6ae3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_42083c354c2b4151ab43ca0f1c4b8eee","IPY_MODEL_55477b119ad14c638ab7f4199e50f277","IPY_MODEL_e34c818f3fea4ab1abb9393269b2a5b8"],"layout":"IPY_MODEL_3dad275398774ab9a13e8fb6ad042ffb"}},"42083c354c2b4151ab43ca0f1c4b8eee":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_82e8ea7f84c14c8cac445da5562a22f7","placeholder":"‚Äã","style":"IPY_MODEL_93145fc50bc74dba8edac352dcd1b769","value":"added_tokens.json:‚Äá100%"}},"55477b119ad14c638ab7f4199e50f277":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f990dadcb7c046dc9a991c6571cc9cab","max":707,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ab2332a490ad4c9eae488afa6e56ea62","value":707}},"e34c818f3fea4ab1abb9393269b2a5b8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0ed1d73e921a4f3c90590eb69ea00297","placeholder":"‚Äã","style":"IPY_MODEL_0412d9e0862548f88e90e8f308f6e0d3","value":"‚Äá707/707‚Äá[00:00&lt;00:00,‚Äá66.2kB/s]"}},"3dad275398774ab9a13e8fb6ad042ffb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"82e8ea7f84c14c8cac445da5562a22f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93145fc50bc74dba8edac352dcd1b769":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f990dadcb7c046dc9a991c6571cc9cab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab2332a490ad4c9eae488afa6e56ea62":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0ed1d73e921a4f3c90590eb69ea00297":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0412d9e0862548f88e90e8f308f6e0d3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"62951fd543bf43d1b8ec140a60825144":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8f1278c561724cc9846487223bc02233","IPY_MODEL_9f1679d7d0be44d9a581610623fee60b","IPY_MODEL_35f71ed727bc41c18868a28c6b8c9978"],"layout":"IPY_MODEL_a89f88bd7dde4f36bdbed5bbc74c7747"}},"8f1278c561724cc9846487223bc02233":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6ef41d823fbf48ceb5afcba0982d4527","placeholder":"‚Äã","style":"IPY_MODEL_d3231c9b9fe047cca10dfe7296154994","value":"special_tokens_map.json:‚Äá100%"}},"9f1679d7d0be44d9a581610623fee60b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_668b2031ff054d19bcaf44e1886665c1","max":614,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c71e6aa0f684488993bdfe2e6682fcc4","value":614}},"35f71ed727bc41c18868a28c6b8c9978":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e680f3d9b534c5eae1db04194c25997","placeholder":"‚Äã","style":"IPY_MODEL_83aea9d305d3496d8c97003642c5b21e","value":"‚Äá614/614‚Äá[00:00&lt;00:00,‚Äá66.8kB/s]"}},"a89f88bd7dde4f36bdbed5bbc74c7747":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ef41d823fbf48ceb5afcba0982d4527":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d3231c9b9fe047cca10dfe7296154994":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"668b2031ff054d19bcaf44e1886665c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c71e6aa0f684488993bdfe2e6682fcc4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7e680f3d9b534c5eae1db04194c25997":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"83aea9d305d3496d8c97003642c5b21e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fae443e4629f4fceaf0b30cf2162b7d9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_94c8ca9f7ca14de08077ade8a90e872d","IPY_MODEL_6e66c53b55104bafade1eab1563ae59e","IPY_MODEL_f90c3aea352f434d960e11a786fe28e9"],"layout":"IPY_MODEL_681eeabc065648b396b6dd4106ea6efa"}},"94c8ca9f7ca14de08077ade8a90e872d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2bae7d7897884694a48038247d1bb356","placeholder":"‚Äã","style":"IPY_MODEL_7d97fd07466e4756b58f54339cf546bc","value":"tokenizer.json:‚Äá100%"}},"6e66c53b55104bafade1eab1563ae59e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5f6a0e3d94f14b11aea2bfc197df8fec","max":11422654,"min":0,"orientation":"horizontal","style":"IPY_MODEL_88697384140b43fb8014e5594e1fbfa6","value":11422654}},"f90c3aea352f434d960e11a786fe28e9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cdad7019e010412ba6f060b419ba2918","placeholder":"‚Äã","style":"IPY_MODEL_d8b7f98057df4492bd64a0a836e27562","value":"‚Äá11.4M/11.4M‚Äá[00:00&lt;00:00,‚Äá35.2MB/s]"}},"681eeabc065648b396b6dd4106ea6efa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2bae7d7897884694a48038247d1bb356":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d97fd07466e4756b58f54339cf546bc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5f6a0e3d94f14b11aea2bfc197df8fec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"88697384140b43fb8014e5594e1fbfa6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cdad7019e010412ba6f060b419ba2918":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d8b7f98057df4492bd64a0a836e27562":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fee33c38e1fb4d32a43ee5fbe5c73d6e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0131c8bc4d70458ab07f958947984e27","IPY_MODEL_90eea63ca2b640908be81791cacda4a9","IPY_MODEL_65cc8560d7ca4b59b6a2a925390622de"],"layout":"IPY_MODEL_152c3d867b3b4340a519846850857c58"}},"0131c8bc4d70458ab07f958947984e27":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_56842f2394854dd4b589cee386900a7c","placeholder":"‚Äã","style":"IPY_MODEL_8754775a57c241a081e70e197d44a23c","value":"chat_template.jinja:‚Äá100%"}},"90eea63ca2b640908be81791cacda4a9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b55659d5c8964096b6d508ea555725f2","max":4673,"min":0,"orientation":"horizontal","style":"IPY_MODEL_90f11492a4db468c9e6f7a9fc489a6b2","value":4673}},"65cc8560d7ca4b59b6a2a925390622de":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a86199929f54638a4f0af40f174e857","placeholder":"‚Äã","style":"IPY_MODEL_8c9a665657374bbbae4b345fc526b6af","value":"‚Äá4.67k/4.67k‚Äá[00:00&lt;00:00,‚Äá506kB/s]"}},"152c3d867b3b4340a519846850857c58":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"56842f2394854dd4b589cee386900a7c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8754775a57c241a081e70e197d44a23c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b55659d5c8964096b6d508ea555725f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90f11492a4db468c9e6f7a9fc489a6b2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5a86199929f54638a4f0af40f174e857":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c9a665657374bbbae4b345fc526b6af":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b28b209294f74d029dfbb5e19ed7fd75":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bd298eb11d8e476fa5d84aab9204bb4a","IPY_MODEL_f81f7c18405c4fd9b5072cce655c35ff","IPY_MODEL_7aa5baf113254adea94108f6390629a1"],"layout":"IPY_MODEL_9ae6f04136054da093110c54ce2cace0"}},"bd298eb11d8e476fa5d84aab9204bb4a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_127c6e3bcef846bfb40f205849d6fcaa","placeholder":"‚Äã","style":"IPY_MODEL_44bf5b85bcc646eeb579e1f6ad8be33e","value":"Unsloth:‚ÄáTokenizing‚Äá[&quot;text&quot;]‚Äá(num_proc=2):‚Äá100%"}},"f81f7c18405c4fd9b5072cce655c35ff":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_41a562878451486aba85b67f281ac7ad","max":752,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cdbc66185d424fe98c90a468a46e70ff","value":752}},"7aa5baf113254adea94108f6390629a1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d567c1e88b94b6c8318d198277f6a7f","placeholder":"‚Äã","style":"IPY_MODEL_0a9f5992ac7147ddaeabf14b8d0c877d","value":"‚Äá752/752‚Äá[00:05&lt;00:00,‚Äá158.41‚Äáexamples/s]"}},"9ae6f04136054da093110c54ce2cace0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"127c6e3bcef846bfb40f205849d6fcaa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"44bf5b85bcc646eeb579e1f6ad8be33e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"41a562878451486aba85b67f281ac7ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cdbc66185d424fe98c90a468a46e70ff":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4d567c1e88b94b6c8318d198277f6a7f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0a9f5992ac7147ddaeabf14b8d0c877d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}